"""Generate interpreted Markdown pages for extracted JSON data files."""

from __future__ import annotations

import json
import os
import re
from collections import defaultdict
from typing import Any

import scripts.config as config


MANUAL_BLOCK_RE = re.compile(r"<!-- MANUAL:START -->.*?<!-- MANUAL:END -->", re.DOTALL)
MAX_GENERIC_ROWS = 220

SEMANTIC_GROUP_ORDER = [
    "Timing & Decay",
    "Thresholds & Bounds",
    "Weights & Multipliers",
    "Probabilities",
    "Stress & Emotion",
    "Identifiers & Labels",
    "Other Parameters",
]


def _group_display(group: str) -> str:
    mapping = {
        "Timing & Decay": "Timing & Decay (ì‹œê°„/ê°ì‡ )",
        "Thresholds & Bounds": "Thresholds & Bounds (ì„ê³„/ê²½ê³„)",
        "Weights & Multipliers": "Weights & Multipliers (ê°€ì¤‘/ë°°ìˆ˜)",
        "Probabilities": "Probabilities (í™•ë¥ )",
        "Stress & Emotion": "Stress & Emotion (ìŠ¤íŠ¸ë ˆìŠ¤/ê°ì •)",
        "Identifiers & Labels": "Identifiers & Labels (ì‹ë³„ì/ë¼ë²¨)",
        "Other Parameters": "Other Parameters (ê¸°íƒ€)",
    }
    return mapping.get(group, group)


def _json_type(value: Any) -> str:
    if isinstance(value, bool):
        return "boolean"
    if isinstance(value, dict):
        return "object"
    if isinstance(value, list):
        return "array"
    if isinstance(value, str):
        return "string"
    if isinstance(value, int):
        return "int"
    if isinstance(value, float):
        return "float"
    if value is None:
        return "null"
    return type(value).__name__


def _escape_pipes(text: str) -> str:
    return text.replace("|", r"\|")


def _sanitize_inline(value: str) -> str:
    return value.replace("\n", " ").strip()


def _to_posix(path: str) -> str:
    return path.replace(os.sep, "/")


def _slug_from_path(file_path: str) -> str:
    return os.path.splitext(os.path.basename(file_path))[0]


def _quote_yaml(text: str) -> str:
    return text.replace("\\", "\\\\").replace('"', r'\"')


def _load_json(path: str) -> Any:
    with open(path, "r", encoding="utf-8") as handle:
        return json.load(handle)


def _entry_content(entry: dict[str, Any]) -> Any:
    """Return the best-effort content payload for a discovered data file entry."""
    for key in ("full_content", "content", "parsed", "data"):
        if key in entry:
            return entry.get(key)
    return None


def _is_meta_key(key: str) -> bool:
    return key.startswith("_comment") or key.startswith("_")


def _is_comment_separator_entry(value: Any) -> bool:
    return isinstance(value, dict) and "_comment" in value and "id" not in value


def _filter_meta_content(value: Any) -> Any:
    if isinstance(value, dict):
        filtered: dict[str, Any] = {}
        for key, item in value.items():
            if _is_meta_key(key):
                continue
            filtered[key] = _filter_meta_content(item)
        return filtered

    if isinstance(value, list):
        filtered_items: list[Any] = []
        for item in value:
            if _is_comment_separator_entry(item):
                continue
            filtered_items.append(_filter_meta_content(item))
        return filtered_items

    return value


def _format_number(value: float) -> str:
    if value.is_integer():
        return str(int(value))
    return f"{value:.4f}".rstrip("0").rstrip(".")


def _infer_unit(parameter: str, value: Any) -> str:
    if isinstance(value, bool) or not isinstance(value, (int, float)):
        return ""

    key = parameter.lower()
    if "hour" in key or key.endswith("_h"):
        return "hours"
    if "day" in key or key.endswith("_d"):
        return "days"
    if "prob" in key or "chance" in key:
        return "probability"
    if "half_life" in key:
        return "hours"
    if "rate" in key:
        return "rate"
    if "threshold" in key or key.endswith("_min") or key.endswith("_max"):
        return "threshold"
    if "weight" in key or "mult" in key or "coeff" in key or "factor" in key or "ratio" in key:
        return "multiplier"
    return ""


def _format_value(value: Any, parameter: str = "") -> str:
    if isinstance(value, str):
        return value
    if isinstance(value, bool):
        return "true" if value else "false"
    if value is None:
        return "null"
    if isinstance(value, (int, float)) and not isinstance(value, bool):
        number_text = _format_number(float(value))
        unit = _infer_unit(parameter, value)
        if unit:
            return f"{number_text} {unit}"
        return number_text
    if isinstance(value, list):
        return f"{len(value)} items"
    if isinstance(value, dict):
        return f"{len(value)} keys"
    return str(value)


def _semantic_group(parameter: str) -> str:
    key = parameter.lower()

    if any(token in key for token in ("half_life", "decay", "duration", "time", "tick", "tau", "season")):
        return "Timing & Decay"
    if any(token in key for token in ("threshold", "min", "max", "limit", "bound", "clamp")):
        return "Thresholds & Bounds"
    if any(token in key for token in ("weight", "mult", "coeff", "factor", "ratio", "scale", "kappa", "gamma", "beta")):
        return "Weights & Multipliers"
    if any(token in key for token in ("chance", "prob", "likelihood")):
        return "Probabilities"
    if any(token in key for token in ("stress", "emotion", "fear", "joy", "anger", "sadness", "trust", "disgust")):
        return "Stress & Emotion"
    if any(token in key for token in ("id", "name", "label", "title", "type", "category")):
        return "Identifiers & Labels"
    return "Other Parameters"


def _infer_control_description(parameter: str) -> str:
    key = parameter.lower()

    # Mental break domain keys
    if "duration_base" in key:
        return "ê¸°ë³¸ ì§€ì† ì‹œê°„ (í‹±) â€” mental break ì§€ì† í‹± ìˆ˜ ê¸°ì¤€ê°’. Base duration in ticks for this mental break type."
    if "duration_variance" in key:
        return "ì§€ì† ì‹œê°„ ë¶„ì‚° (í‹±) â€” ê¸°ì¤€ê°’ì— ë”í•´ì§€ëŠ” ë¬´ì‘ìœ„ ë¶„ì‚° ë²”ìœ„. Random variance range added to base duration."
    if "break_scale" in key:
        return "ë°œë™ í™•ë¥  ë¶„ëª¨ â€” p = (stress - threshold) / break_scale. Higher = rarer breaks. í™•ë¥  ê³„ì‚° ë¶„ëª¨."
    if "break_cap" in key or "cap_per_tick" in key:
        return "í‹±ë‹¹ ìµœëŒ€ ë°œë™ í™•ë¥  ìƒí•œ. Maximum mental break probability per simulation tick."
    if "shaken_work_penalty" in key:
        return "í”ë“¤ë¦° ìƒíƒœ ì‘ì—… íš¨ìœ¨ íŒ¨ë„í‹°. Work efficiency penalty applied during the shaken aftermath state."
    if "shaken_duration" in key:
        return "í”ë“¤ë¦° ìƒíƒœ ì§€ì† ì‹œê°„ (í‹±). Duration of shaken aftermath state in simulation ticks."
    if "catharsis" in key:
        return "ì¹´íƒ€ë¥´ì‹œìŠ¤ íšŒë³µ ë¹„ìœ¨ â€” mental break ì¢…ë£Œ í›„ ìŠ¤íŠ¸ë ˆìŠ¤ íšŒë³µ ê³„ìˆ˜. Stress recovery factor on break resolution."
    if "weight" in key and (
        "break" in key
        or any(
            bt in key
            for bt in (
                "panic",
                "rage",
                "grief",
                "fugue",
                "outrage",
                "paranoia",
                "purge",
                "ritual",
                "bonding",
                "shutdown",
            )
        )
    ):
        return "ì´ ìœ í˜•ì˜ mental break ì„ íƒ ê°€ì¤‘ì¹˜. Selection weight for this break type during break selection."
    if "contagion" in key:
        return "ê°ì • ì „íŒŒ ê³„ìˆ˜ (Îº) â€” ì£¼ë³€ ì—”í‹°í‹°ë¡œì˜ ê°ì • ì „íŒŒ ê°•ë„. Emotional contagion coefficient to nearby entities."
    if "duration" in key and "ticks" in key:
        return "ì§€ì† ì‹œê°„ (í‹±). Duration in simulation ticks."

    if "siler" in key:
        return "Siler mortality hazard component. (Siler ì‚¬ë§ ìœ„í—˜ë„ êµ¬ì„±ìš”ì†Œ)"
    if "care_protection" in key:
        return "Infant protection modifier applied when caregivers can respond. (ì˜ì•„ ë³´í˜¸ ë³´ì •ê°’)"
    if "tech_modifiers" in key:
        return "Technology-driven reduction of mortality pressure. (ê¸°ìˆ  ìˆ˜ì¤€ì— ë”°ë¥¸ ì‚¬ë§ë¥  ê°ì†Œ)"
    if "season_modifiers" in key:
        return "Seasonal environmental multiplier applied to mortality risk. (ê³„ì ˆ í™˜ê²½ ë°°ìˆ˜)"
    if any(token in key for token in ("half_life", "decay", "tau")):
        return "How quickly this state fades over simulation time. (ì‹œê°„ ê²½ê³¼ ê°ì†Œ ì†ë„)"
    if any(token in key for token in ("threshold", "min", "max", "limit", "bound")):
        return "Activation boundary used by game logic. (ì‘ë™ ì„ê³„ê°’)"
    if any(token in key for token in ("weight", "mult", "coeff", "factor", "ratio", "scale", "kappa", "gamma", "beta")):
        return "Strength multiplier used in gameplay calculations. (ê³„ì‚° ê°•ë„ ë°°ìˆ˜)"
    if any(token in key for token in ("chance", "prob", "likelihood")):
        return "Probability that this branch triggers during evaluation. (ë°œìƒ í™•ë¥ )"
    if any(token in key for token in ("stress", "emotion", "fear", "joy", "anger", "sadness", "trust", "disgust")):
        return "Stress/emotion contribution in simulation updates. (ìŠ¤íŠ¸ë ˆìŠ¤/ê°ì • ê¸°ì—¬ë„)"
    if any(token in key for token in ("id", "name", "label", "title", "type", "category")):
        return "Identifier/label used for lookup or UI presentation. (ì‹ë³„ì/ë¼ë²¨)"
    return "General configuration parameter used by the corresponding system. (í•´ë‹¹ ì‹œìŠ¤í…œì˜ ì¼ë°˜ ì„¤ì • ê°’)"


def _flatten_parameters(value: Any, prefix: str = "", rows: list[tuple[str, Any]] | None = None) -> list[tuple[str, Any]]:
    if rows is None:
        rows = []

    if len(rows) >= MAX_GENERIC_ROWS:
        return rows

    if isinstance(value, dict):
        for key in sorted(value.keys()):
            child = value[key]
            path = f"{prefix}.{key}" if prefix else str(key)
            if isinstance(child, dict):
                _flatten_parameters(child, path, rows)
                if len(rows) >= MAX_GENERIC_ROWS:
                    break
                continue
            if isinstance(child, list):
                rows.append((path, child))
                if child and isinstance(child[0], dict):
                    _flatten_parameters(child[0], f"{path}.item", rows)
                elif child:
                    rows.append((f"{path}.sample", child[0]))
                if len(rows) >= MAX_GENERIC_ROWS:
                    break
                continue
            rows.append((path, child))
            if len(rows) >= MAX_GENERIC_ROWS:
                break
        return rows

    if isinstance(value, list):
        rows.append((prefix or "items", value))
        if value:
            first = value[0]
            if isinstance(first, dict):
                _flatten_parameters(first, f"{prefix}.item" if prefix else "item", rows)
            else:
                rows.append((f"{prefix}.sample" if prefix else "sample", first))
        return rows

    if prefix:
        rows.append((prefix, value))
    return rows


def _render_parameter_table(rows: list[tuple[str, str, str, str]]) -> str:
    lines = [
        "| Parameter (ë§¤ê°œë³€ìˆ˜) | Value (ê°’) | Type (ìœ í˜•) | What it controls (ê²Œì„ ì˜í–¥) |",
        "|----------------------|-----------|------------|-----------------------------|",
    ]

    for parameter, value, type_name, controls in rows:
        safe_parameter = _escape_pipes(_sanitize_inline(parameter))
        safe_value = _escape_pipes(_sanitize_inline(value))
        safe_type = _escape_pipes(_sanitize_inline(type_name))
        safe_controls = _escape_pipes(_sanitize_inline(controls))
        lines.append(
            f"| `{safe_parameter}` | {safe_value} | {safe_type} | {safe_controls} |"
        )

    return "\n".join(lines)


def _render_metric_table(rows: list[tuple[str, str]]) -> str:
    lines = [
        "| Metric (ì§€í‘œ) | Value (ê°’) |",
        "|---------------|-----------|",
    ]

    for metric, value in rows:
        safe_metric = _escape_pipes(_sanitize_inline(metric))
        safe_value = _escape_pipes(_sanitize_inline(value))
        lines.append(f"| {safe_metric} | {safe_value} |")

    return "\n".join(lines)


def _module_slug(section: str, module_file: str, module_entry: dict[str, Any]) -> str:
    if section in {"systems", "ai_modules"}:
        system_name = module_entry.get("system_name")
        if isinstance(system_name, str) and system_name:
            return system_name
        stem = _slug_from_path(module_file)
        return stem[:-7] if stem.endswith("_system") else stem
    return _slug_from_path(module_file)


def _build_module_meta(manifest: dict) -> dict[str, dict[str, str]]:
    meta: dict[str, dict[str, str]] = {}
    sections = (
        ("systems", config.CONTENT_SYSTEMS),
        ("ai_modules", config.CONTENT_SYSTEMS),
        ("core_modules", config.CONTENT_CORE),
    )

    for section, content_dir in sections:
        entries = manifest.get(section, [])
        if not isinstance(entries, list):
            continue

        for module_entry in entries:
            if not isinstance(module_entry, dict):
                continue

            file_path = module_entry.get("file")
            if not isinstance(file_path, str) or not file_path.endswith(".gd"):
                continue

            slug = _module_slug(section, file_path, module_entry)
            title = (
                module_entry.get("system_name")
                if section in {"systems", "ai_modules"}
                else _slug_from_path(file_path)
            )
            if not isinstance(title, str) or not title:
                title = _slug_from_path(file_path)

            meta[file_path] = {
                "title": title,
                "doc_path": os.path.join(content_dir, f"{slug}.md"),
            }

    return meta


def _dependency_matches_file(dependency: str, data_file: str) -> bool:
    dep = dependency.replace("\\", "/").rstrip("/")
    target = data_file.replace("\\", "/")

    if dep == target:
        return True

    return target.startswith(dep + "/")


def _build_referenced_by_map(
    data_entries: list[dict[str, Any]],
    manifest: dict,
    warnings: list[str],
) -> dict[str, list[dict[str, str]]]:
    references_path = os.path.join(config.EXTRACTED_DIR, "references.json")
    if not os.path.exists(references_path):
        warnings.append("references.json not found; Referenced By sections omitted")
        return {}

    try:
        references = _load_json(references_path)
    except (OSError, json.JSONDecodeError) as exc:
        warnings.append(f"failed to load references.json: {exc}")
        return {}

    dependency_graph = references.get("dependency_graph")
    if not isinstance(dependency_graph, dict):
        warnings.append("references.json missing dependency_graph; Referenced By sections omitted")
        return {}

    module_meta = _build_module_meta(manifest)
    referenced_by: dict[str, list[dict[str, str]]] = defaultdict(list)

    data_files: list[str] = []
    for entry in data_entries:
        file_path = entry.get("file")
        if isinstance(file_path, str) and file_path:
            data_files.append(file_path)

    for module_file, module_data in sorted(dependency_graph.items()):
        if not isinstance(module_data, dict):
            continue

        depends_on = module_data.get("depends_on")
        if not isinstance(depends_on, list):
            continue

        module_info = module_meta.get(module_file)
        if not module_info:
            continue

        for dep in depends_on:
            if not isinstance(dep, str) or not dep.startswith("data/"):
                continue

            for data_file in data_files:
                if not _dependency_matches_file(dep, data_file):
                    continue

                reason = f"references `{dep}`" if data_file == dep else f"references data under `{dep.rstrip('/')}/`"
                referenced_by[data_file].append(
                    {
                        "module_title": module_info["title"],
                        "module_doc_path": module_info["doc_path"],
                        "reason": reason,
                    }
                )

    for file_path, refs in referenced_by.items():
        deduped: dict[tuple[str, str], dict[str, str]] = {}
        for ref in refs:
            key = (ref["module_title"], ref["module_doc_path"])
            deduped[key] = ref
        referenced_by[file_path] = [deduped[key] for key in sorted(deduped.keys())]

    return referenced_by


def _preserve_manual_blocks(path: str, new_text: str) -> str:
    if not os.path.exists(path):
        return new_text

    try:
        with open(path, "r", encoding="utf-8") as handle:
            previous_text = handle.read()
    except OSError:
        return new_text

    old_blocks = MANUAL_BLOCK_RE.findall(previous_text)
    if not old_blocks:
        return new_text

    if not MANUAL_BLOCK_RE.search(new_text):
        return new_text

    index = 0

    def _replace_block(match: re.Match[str]) -> str:
        nonlocal index
        if index >= len(old_blocks):
            return match.group(0)
        block = old_blocks[index]
        index += 1
        return block

    return MANUAL_BLOCK_RE.sub(_replace_block, new_text)


def _write_markdown(path: str, content: str, files_written: list[str], errors: list[str]) -> None:
    try:
        with open(path, "w", encoding="utf-8") as handle:
            handle.write(content)
        files_written.append(path)
    except OSError as exc:
        errors.append(f"failed to write {path}: {exc}")


def _derive_domain_from_file(file_path: str) -> str:
    normalized = file_path.replace("\\", "/")
    if normalized.startswith("data/species/"):
        return "species"
    if "/emotions/" in normalized or normalized.startswith("data/emotions/"):
        return "emotions"
    if "/personality/" in normalized or normalized.startswith("data/personality/"):
        return "personality"
    if normalized.endswith("stressor_events.json"):
        return "stress"

    parts = normalized.split("/")
    if len(parts) >= 2 and parts[0] == "data":
        return parts[1]
    return "unknown"


def _domain_group(entry: dict[str, Any]) -> str:
    file_path = entry.get("file", "")
    if not isinstance(file_path, str):
        return "unknown"

    domain = entry.get("domain")
    if not isinstance(domain, str) or not domain:
        domain = _derive_domain_from_file(file_path)

    normalized = domain.lower()
    if normalized in {"species", "emotions", "personality", "stress"}:
        return normalized
    if normalized == "mortality":
        return "species"
    return normalized


def _domain_display(group: str) -> str:
    mapping = {
        "species": "Species (ì¢…ì¡±)",
        "emotions": "Emotions (ê°ì •)",
        "personality": "Personality (ì„±ê²©)",
        "stress": "Stress (ìŠ¤íŠ¸ë ˆìŠ¤)",
        "unknown": "Unknown (ë¯¸ìƒ)",
    }
    return mapping.get(group, group.replace("_", " ").title())


def _load_optional_payload(extracted: dict[str, Any], key: str, warnings: list[str]) -> dict[str, Any] | None:
    value = extracted.get(key)
    if isinstance(value, dict):
        return value

    fallback_file = {
        "trait_data": "trait_data.json",
        "emotion_presets": "emotion_presets.json",
        "decay_config": "decay_config.json",
        "stressor_data": "stressor_data.json",
    }.get(key)
    if not fallback_file:
        return None

    path = os.path.join(config.EXTRACTED_DIR, fallback_file)
    if not os.path.exists(path):
        return None

    try:
        loaded = _load_json(path)
    except (OSError, json.JSONDecodeError) as exc:
        warnings.append(f"failed to load {fallback_file}: {exc}")
        return None

    if not isinstance(loaded, dict):
        warnings.append(f"{fallback_file} is not an object; ignoring")
        return None
    return loaded


def _resolve_data_entries(
    extracted: dict[str, Any],
    warnings: list[str],
    errors: list[str],
) -> list[dict[str, Any]]:
    in_memory = extracted.get("data_files")
    if isinstance(in_memory, dict):
        files = in_memory.get("files")
        if isinstance(files, list):
            return [item for item in files if isinstance(item, dict)]
    elif isinstance(in_memory, list):
        return [item for item in in_memory if isinstance(item, dict)]

    extracted_path = os.path.join(config.EXTRACTED_DIR, "data_files.json")
    if not os.path.exists(extracted_path):
        warnings.append("extracted/data_files.json not found")
        return []

    try:
        loaded = _load_json(extracted_path)
    except (OSError, json.JSONDecodeError) as exc:
        errors.append(f"failed to load extracted/data_files.json: {exc}")
        return []

    files = loaded.get("files") if isinstance(loaded, dict) else None
    if not isinstance(files, list):
        warnings.append("extracted/data_files.json has non-list 'files' field")
        return []

    return [item for item in files if isinstance(item, dict)]


def _known_system_docs(file_stem: str) -> list[tuple[str, str]]:
    mapping: dict[str, list[tuple[str, str]]] = {
        "siler_parameters": [("mortality", os.path.join(config.CONTENT_SYSTEMS, "mortality.md"))],
        "trait_definitions": [("trait", os.path.join(config.CONTENT_SYSTEMS, "trait.md"))],
        "decay_parameters": [("emotions", os.path.join(config.CONTENT_SYSTEMS, "emotions.md"))],
        "event_presets": [("emotions", os.path.join(config.CONTENT_SYSTEMS, "emotions.md"))],
        "stressor_events": [("stress", os.path.join(config.CONTENT_SYSTEMS, "stress.md"))],
    }
    return mapping.get(file_stem, [])


def _overview_summary(file_stem: str, category: str) -> str:
    mapping = {
        "siler_parameters": "ì¢…ì¡± ì‚¬ë§ë¥  ìœ„í—˜ë„ì™€ ìƒì¡´ ë³´ì •ê°’. Species mortality hazards and survival modifiers.",
        "trait_definitions": "ì„±ê²© íŠ¹ì„±ì˜ ë°œë™ ê·œì¹™ê³¼ íš¨ê³¼ ë©”íƒ€ë°ì´í„°. Trait activation rules and effect metadata.",
        "decay_parameters": "ê°ì • ê°ì‡ /ì „ì—¼/ê¸°ì¤€ì¹˜/ì •ì‹  ë¶•ê´´ íŠœë‹. Emotion decay, contagion, baselines, mental-break tuning.",
        "event_presets": "ì„¸ê³„ ì´ë²¤íŠ¸ë¥¼ ê°ì • ìê·¹ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” í‰ê°€ í”„ë¦¬ì…‹. Event appraisal presets for emotion impulses.",
        "stressor_events": "ìŠ¤íŠ¸ë ˆìŠ¤ ì´ë²¤íŠ¸ í…œí”Œë¦¿ê³¼ ì‹¬ê°ë„/ë§¥ë½ ë³´ì •. Stress event templates and severity/context modifiers.",
    }
    if file_stem in mapping:
        return mapping[file_stem]
    return f"`{category}` ë°ì´í„° ë„ë©”ì¸ ì„¤ì • ê°’. Configuration values for the `{category}` data domain."


def _relative_link(output_dir: str, target_path: str) -> str:
    return _to_posix(os.path.relpath(target_path, output_dir))


def _render_overview_section(
    *,
    entry: dict[str, Any],
    referenced_by: list[dict[str, str]],
    output_dir: str,
) -> list[str]:
    source_file = entry["file"]
    category = entry["category"]
    file_stem = _slug_from_path(source_file)

    lines = [
        "## ê°œìš” (Overview)",
        "",
        f"- Configures (ì„¤ì • ë‚´ìš©): {_overview_summary(file_stem, category)}",
    ]

    if referenced_by:
        reader_names = ", ".join(sorted({ref["module_title"] for ref in referenced_by}))
        lines.append(f"- Read by systems/modules (ì½ëŠ” ì‹œìŠ¤í…œ/ëª¨ë“ˆ): {reader_names}")
    else:
        lines.append("- Read by systems/modules (ì½ëŠ” ì‹œìŠ¤í…œ/ëª¨ë“ˆ): `references.json`ì—ì„œ ì¶”ë¡ ë˜ì§€ ì•ŠìŒ.")

    doc_links: list[str] = []
    for title, doc_path in _known_system_docs(file_stem):
        doc_links.append(f"[`{title}`]({_relative_link(output_dir, doc_path)})")

    if not doc_links and referenced_by:
        for ref in referenced_by[:3]:
            doc_links.append(
                f"[`{ref['module_title']}`]({_relative_link(output_dir, ref['module_doc_path'])})"
            )

    if doc_links:
        lines.append(f"- Related documentation (ê´€ë ¨ ë¬¸ì„œ): {', '.join(doc_links)}")
    else:
        lines.append("- Related documentation (ê´€ë ¨ ë¬¸ì„œ): ì—†ìŒ.")

    return lines + [""]


def _academic_references(file_stem: str) -> list[str]:
    mapping = {
        "siler_parameters": ["Siler (1979) Competing-Risk Model"],
        "decay_parameters": ["Verduyn & Brans (2012)", "Plutchik (1980)"],
        "event_presets": ["Plutchik (1980)", "Lazarus (1991)", "Scherer (2009)"],
        "stressor_events": ["Holmes & Rahe (1967)", "Hobfoll (1989) COR"],
    }
    return mapping.get(file_stem, [])


def _render_siler_parameters(content: dict[str, Any]) -> list[str]:
    baseline = content.get("baseline") if isinstance(content.get("baseline"), dict) else {}
    tech_modifiers = content.get("tech_modifiers") if isinstance(content.get("tech_modifiers"), dict) else {}
    care_protection = content.get("care_protection") if isinstance(content.get("care_protection"), dict) else {}
    season_modifiers = content.get("season_modifiers") if isinstance(content.get("season_modifiers"), dict) else {}

    baseline_help = {
        "a1": "Infant mortality amplitude in the Siler hazard term. (ì˜ì•„ ì‚¬ë§ ìœ„í—˜ë„ ì§„í­)",
        "b1": "Rate at which infant mortality hazard declines with age. (ì—°ë ¹ì— ë”°ë¥¸ ì˜ì•„ ìœ„í—˜ë„ ê°ì†Œìœ¨)",
        "a2": "Age-independent background mortality floor. (ì—°ë ¹ ë¬´ê´€ ê¸°ë³¸ ì‚¬ë§ë¥ )",
        "a3": "Late-life mortality growth amplitude. (ë…¸ë…„ê¸° ìœ„í—˜ë„ ì§„í­)",
        "b3": "Exponential growth rate of senescent mortality. (ë…¸ë…„ê¸° ìœ„í—˜ë„ ì¦ê°€ìœ¨)",
    }

    tech_help = {
        "k1": "Tech-driven reduction for infant hazard; lower hazard â‡’ higher survival. (ê¸°ìˆ ë¡œ ì˜ì•„ ìœ„í—˜ë„ ê°ì†Œ â†’ ìƒì¡´ìœ¨ ì¦ê°€)",
        "k2": "Tech-driven reduction for background hazard; lower hazard â‡’ higher survival. (ê¸°ìˆ ë¡œ ê¸°ë³¸ ìœ„í—˜ë„ ê°ì†Œ â†’ ìƒì¡´ìœ¨ ì¦ê°€)",
        "k3": "Tech-driven reduction for senescent hazard; lower hazard â‡’ higher survival. (ê¸°ìˆ ë¡œ ë…¸ë…„ ìœ„í—˜ë„ ê°ì†Œ â†’ ìƒì¡´ìœ¨ ì¦ê°€)",
    }

    care_help = {
        "hunger_min": "Minimum hunger condition where infant care protection remains active. (ë³´í˜¸ ìœ ì§€ ìµœì†Œ ë°°ê³ í””)",
        "protection_factor": "Fraction of infant mortality risk reduced by effective care. (ë³´í˜¸ë¡œ ì¤„ì–´ë“œëŠ” ìœ„í—˜ ë¹„ìœ¨)",
    }

    lines = [
        "## ì‚¬ë§ ëª¨ë¸ í•´ì„ (Mortality Model Interpretation)",
        "",
        "This file defines parameters for the Siler competing-risk hazard model used by the mortality system. (ì‚¬ë§ ì‹œìŠ¤í…œì´ ì‚¬ìš©í•˜ëŠ” Siler ê²½ìŸìœ„í—˜ë„ ëª¨ë¸ íŒŒë¼ë¯¸í„°)",
        "",
        "Siler hazard (Siler ì‚¬ë§ ìœ„í—˜ë„):",
        "",
        "$$\\mu(x) = a_1 e^{-b_1 x} + a_2 + a_3 e^{b_3 x}$$",
        "",
        "- `a1,b1`: infant/early-life hazard that decays with age. (ì˜ì•„/ì´ˆê¸° ìœ„í—˜ë„, ì—°ë ¹ ì¦ê°€ë¡œ ê°ì†Œ)",
        "- `a2`: background hazard floor. (ì—°ë ¹ ë¬´ê´€ ê¸°ë³¸ ìœ„í—˜ë„)",
        "- `a3,b3`: senescent hazard that grows with age. (ë…¸ë…„ê¸° ìœ„í—˜ë„, ì—°ë ¹ ì¦ê°€ë¡œ ì¦ê°€)",
        "",
        "### ê¸°ë³¸ íŒŒë¼ë¯¸í„° (Baseline Parameters)",
        "",
    ]

    baseline_rows: list[tuple[str, str, str, str]] = []
    for key in sorted(baseline.keys()):
        value = baseline[key]
        baseline_rows.append(
            (
                key,
                _format_value(value, key),
                _json_type(value),
                baseline_help.get(key, _infer_control_description(key)),
            )
        )

    if baseline_rows:
        lines.extend([_render_parameter_table(baseline_rows), ""])
    else:
        lines.extend(["- No baseline parameters found. (ê¸°ë³¸ íŒŒë¼ë¯¸í„° ì—†ìŒ)", ""])

    lines.extend(["### ê¸°ìˆ  ë³´ì • (Technology Modifiers)", ""])
    tech_rows: list[tuple[str, str, str, str]] = []
    for key in sorted(tech_modifiers.keys()):
        value = tech_modifiers[key]
        tech_rows.append((f"tech_modifiers.{key}", _format_value(value, key), _json_type(value), tech_help.get(key, _infer_control_description(f"tech_modifiers.{key}"))))

    if tech_rows:
        lines.extend([
            "Higher technology typically reduces mortality via these coefficients; lower hazard â‡’ higher survival. (ê¸°ìˆ  ìˆ˜ì¤€ì´ ë†’ì„ìˆ˜ë¡ ìœ„í—˜ë„â†“ â†’ ìƒì¡´ìœ¨â†‘)",
            "",
            _render_parameter_table(tech_rows),
            "",
        ])
    else:
        lines.extend(["- No technology modifiers found. (ê¸°ìˆ  ë³´ì • ì—†ìŒ)", ""])

    lines.extend(["### ì˜ì•„ ë³´í˜¸ (Infant Care Protection)", ""])
    care_rows: list[tuple[str, str, str, str]] = []
    for key in sorted(care_protection.keys()):
        value = care_protection[key]
        care_rows.append((f"care_protection.{key}", _format_value(value, key), _json_type(value), care_help.get(key, _infer_control_description(f"care_protection.{key}"))))

    if care_rows:
        lines.extend([
            "These parameters model caregiver buffering for infant survival during vulnerable periods. (ë³´í˜¸ì ëŒë´„ì— ë”°ë¥¸ ìƒì¡´ ì™„ì¶©)",
            "",
            _render_parameter_table(care_rows),
            "",
        ])
    else:
        lines.extend(["- No care_protection section found. (care_protection ì—†ìŒ)", ""])

    lines.extend(["### ê³„ì ˆ í™˜ê²½ ì˜í–¥ (Seasonal Environment Effects)", ""])
    season_rows: list[tuple[str, str, str, str]] = []
    for season, payload in sorted(season_modifiers.items()):
        if not isinstance(payload, dict):
            season_rows.append((f"season_modifiers.{season}", _format_value(payload, season), _json_type(payload), _infer_control_description(f"season_modifiers.{season}")))
            continue
        for channel, value in sorted(payload.items()):
            parameter = f"season_modifiers.{season}.{channel}"
            if channel == "infant":
                control = f"Seasonal multiplier for infant mortality during {season}. ({season} ì˜ì•„ ì‚¬ë§ë¥  ê³„ì ˆ ë°°ìˆ˜)"
            elif channel == "background":
                control = f"Seasonal multiplier for background mortality during {season}. ({season} ê¸°ë³¸ ì‚¬ë§ë¥  ê³„ì ˆ ë°°ìˆ˜)"
            else:
                control = _infer_control_description(parameter)
            season_rows.append((parameter, _format_value(value, parameter), _json_type(value), control))

    if season_rows:
        lines.extend([_render_parameter_table(season_rows), ""])
    else:
        lines.extend(["- No season_modifiers found. (ê³„ì ˆ ë³´ì • ì—†ìŒ)", ""])

    return lines


def _count_trait_entries(content: Any) -> int:
    if isinstance(content, list):
        return sum(1 for item in content if isinstance(item, dict) and item.get("id"))
    if isinstance(content, dict):
        traits = content.get("traits")
        if isinstance(traits, list):
            return sum(1 for item in traits if isinstance(item, dict) and item.get("id"))
    return 0


def _render_personality_defer(
    *,
    content: Any,
    trait_data: dict[str, Any] | None,
    output_dir: str,
) -> list[str]:
    lines = [
        "## ì „ìš© ë¬¸ì„œ (Specialized Documentation)",
        "",
        "Detailed trait interpretation is generated by `scripts/generators/trait_page.py`. (ì„¸ë¶€ íŠ¹ì„± í•´ì„ì€ ë³„ë„ ìƒì„±)",
        "",
    ]

    trait_index_path = os.path.join(config.CONTENT_DIR, "traits", "_index.md")
    lines.append(
        f"- See [`Trait Pages`]({_relative_link(output_dir, trait_index_path)}) for HEXACO conditions and effect breakdowns. (HEXACO ì¡°ê±´/íš¨ê³¼ ì •ë¦¬)"
    )
    lines.append("")

    total_traits = _count_trait_entries(content)
    axis_count = "-"
    type_breakdown = "-"

    if trait_data:
        stats = trait_data.get("stats") if isinstance(trait_data.get("stats"), dict) else {}
        total_from_extractor = stats.get("total_traits")
        if isinstance(total_from_extractor, int):
            total_traits = total_from_extractor

        by_axis = stats.get("by_axis_count")
        if isinstance(by_axis, dict):
            axis_count = str(len(by_axis))

        by_type_count = stats.get("by_type_count")
        if isinstance(by_type_count, dict) and by_type_count:
            parts = []
            for key in sorted(by_type_count.keys()):
                value = by_type_count[key]
                if isinstance(value, int):
                    parts.append(f"{key}:{value}")
            if parts:
                type_breakdown = ", ".join(parts)

    summary_rows = [
        ("Trait entries (íŠ¹ì„± ìˆ˜)", str(total_traits)),
        ("HEXACO axis groups (ì¶• ê·¸ë£¹)", axis_count),
        ("Type breakdown (ìœ í˜• ë¶„í¬)", type_breakdown),
    ]

    lines.extend([
        "### ìš”ì•½ (Summary)",
        "",
        _render_metric_table(summary_rows),
        "",
    ])

    return lines


def _render_emotion_defer(
    *,
    file_stem: str,
    content: Any,
    output_dir: str,
    emotion_presets: dict[str, Any] | None,
    decay_config: dict[str, Any] | None,
) -> list[str]:
    lines = [
        "## ì „ìš© ë¬¸ì„œ (Specialized Documentation)",
        "",
        "Detailed emotion-system explanation is generated by `scripts/generators/emotion_detail.py`. (ê°ì • ì‹œìŠ¤í…œ ìƒì„¸ ë¬¸ì„œ)",
        "",
    ]

    detail_path = os.path.join(config.CONTENT_SYSTEMS, "emotion-detail.md")
    lines.append(
        f"- See [`Emotion Detail`]({_relative_link(output_dir, detail_path)}) for model-level formulas and dynamics. (ëª¨ë¸ ìˆ˜ì‹/ë™ì—­í•™)"
    )
    lines.append("")

    summary_rows: list[tuple[str, str]] = []

    if file_stem == "decay_parameters":
        stats = decay_config.get("stats") if isinstance(decay_config, dict) and isinstance(decay_config.get("stats"), dict) else {}
        total_parameters = stats.get("total_parameters") if isinstance(stats.get("total_parameters"), int) else "-"
        emotion_count = stats.get("emotion_count") if isinstance(stats.get("emotion_count"), int) else "-"
        mental_break_types = stats.get("mental_break_types") if isinstance(stats.get("mental_break_types"), int) else "-"

        if isinstance(content, dict) and total_parameters == "-":
            total_parameters = len(_flatten_parameters(content))

        summary_rows = [
            ("Total parameters (ì´ íŒŒë¼ë¯¸í„°)", str(total_parameters)),
            ("Emotion channels (ê°ì • ì±„ë„)", str(emotion_count)),
            ("Mental break behavior types (ë¶•ê´´ ìœ í˜•)", str(mental_break_types)),
        ]

    if file_stem == "event_presets":
        stats = emotion_presets.get("stats") if isinstance(emotion_presets, dict) and isinstance(emotion_presets.get("stats"), dict) else {}
        total_presets = stats.get("total_presets") if isinstance(stats.get("total_presets"), int) else 0
        by_category_count = stats.get("by_category_count") if isinstance(stats.get("by_category_count"), dict) else {}
        trauma_events = stats.get("trauma_events") if isinstance(stats.get("trauma_events"), int) else "-"

        if isinstance(content, dict):
            events = content.get("events")
            if isinstance(events, dict):
                total_presets = max(total_presets, len(events))
                if not by_category_count:
                    generated: dict[str, int] = {}
                    for payload in events.values():
                        if not isinstance(payload, dict):
                            continue
                        category = payload.get("category")
                        if not isinstance(category, str) or not category:
                            category = "unknown"
                        generated[category] = generated.get(category, 0) + 1
                    by_category_count = generated

        top_categories = ", ".join(
            f"{key}:{value}"
            for key, value in sorted(by_category_count.items())[:6]
            if isinstance(value, int)
        )
        if not top_categories:
            top_categories = "-"

        summary_rows = [
            ("Total event presets (í”„ë¦¬ì…‹ ìˆ˜)", str(total_presets)),
            ("Trauma-marked events (íŠ¸ë¼ìš°ë§ˆ)", str(trauma_events)),
            ("Category counts (ì¹´í…Œê³ ë¦¬)", top_categories),
        ]

    lines.extend([
        "### ìš”ì•½ (Summary)",
        "",
        _render_metric_table(summary_rows),
        "",
    ])

    return lines


def _render_stress_defer(
    *,
    output_dir: str,
    stressor_data: dict[str, Any] | None,
    content: Any,
) -> list[str]:
    lines = [
        "## ì „ìš© ë¬¸ì„œ (Specialized Documentation)",
        "",
        "Detailed stress mechanics are generated by `scripts/generators/stress_detail.py`. (ìŠ¤íŠ¸ë ˆìŠ¤ ìƒì„¸ ë¬¸ì„œ)",
        "",
    ]

    detail_path = os.path.join(config.CONTENT_SYSTEMS, "stress-detail.md")
    lines.append(
        f"- See [`Stress Detail`]({_relative_link(output_dir, detail_path)}) for pipeline formulas and stage interpretation. (ìˆ˜ì‹/ë‹¨ê³„ í•´ì„)"
    )
    lines.append("")

    stats = stressor_data.get("stats") if isinstance(stressor_data, dict) and isinstance(stressor_data.get("stats"), dict) else {}

    total_events = stats.get("total_events") if isinstance(stats.get("total_events"), int) else 0
    loss_events = stats.get("loss_events") if isinstance(stats.get("loss_events"), int) else "-"
    by_category = stats.get("by_category_count") if isinstance(stats.get("by_category_count"), dict) else {}

    if isinstance(content, dict) and total_events == 0:
        total_events = sum(1 for value in content.values() if isinstance(value, dict))

    category_summary = ", ".join(
        f"{key}:{value}" for key, value in sorted(by_category.items()) if isinstance(value, int)
    )
    if not category_summary:
        category_summary = "-"

    summary_rows = [
        ("Total stressor events (ì´ë²¤íŠ¸ ìˆ˜)", str(total_events)),
        ("Loss-coded events (ìƒì‹¤ ë¶„ë¥˜)", str(loss_events)),
        ("By category (ì¹´í…Œê³ ë¦¬)", category_summary),
    ]

    lines.extend([
        "### ìš”ì•½ (Summary)",
        "",
        _render_metric_table(summary_rows),
        "",
    ])

    return lines


def _render_generic_interpreted(content: Any) -> list[str]:
    lines = [
        "## í•´ì„ëœ íŒŒë¼ë¯¸í„° (Interpreted Parameters)",
        "",
    ]

    grouped_rows: dict[str, list[tuple[str, str, str, str]]] = defaultdict(list)

    flattened = _flatten_parameters(content)
    for parameter, value in flattened:
        group = _semantic_group(parameter)
        grouped_rows[group].append(
            (
                parameter,
                _format_value(value, parameter),
                _json_type(value),
                _infer_control_description(parameter),
            )
        )

    if not grouped_rows:
        lines.extend(["- No interpretable parameters found. (í•´ì„ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ì—†ìŒ)", ""])
        return lines

    for group in SEMANTIC_GROUP_ORDER:
        rows = grouped_rows.get(group, [])
        if not rows:
            continue
        lines.extend([
            f"### {_group_display(group)}",
            "",
            _render_parameter_table(rows),
            "",
        ])

    for group in sorted(grouped_rows.keys()):
        if group in SEMANTIC_GROUP_ORDER:
            continue
        lines.extend([
            f"### {_group_display(group)}",
            "",
            _render_parameter_table(grouped_rows[group]),
            "",
        ])

    return lines


def _render_domain_content(
    *,
    entry: dict[str, Any],
    output_dir: str,
    trait_data: dict[str, Any] | None,
    emotion_presets: dict[str, Any] | None,
    decay_config: dict[str, Any] | None,
    stressor_data: dict[str, Any] | None,
) -> list[str]:
    source_file = entry["file"]
    file_stem = _slug_from_path(source_file)
    content = entry.get("full_content")

    if file_stem == "siler_parameters" and isinstance(content, dict):
        return _render_siler_parameters(content)

    if file_stem == "trait_definitions":
        return _render_personality_defer(content=content, trait_data=trait_data, output_dir=output_dir)

    if file_stem in {"decay_parameters", "event_presets"}:
        return _render_emotion_defer(
            file_stem=file_stem,
            content=content,
            output_dir=output_dir,
            emotion_presets=emotion_presets,
            decay_config=decay_config,
        )

    if file_stem == "stressor_events":
        return _render_stress_defer(output_dir=output_dir, stressor_data=stressor_data, content=content)

    return _render_generic_interpreted(content)


def _render_page(
    *,
    entry: dict[str, Any],
    referenced_by: list[dict[str, str]],
    output_dir: str,
    trait_data: dict[str, Any] | None,
    emotion_presets: dict[str, Any] | None,
    decay_config: dict[str, Any] | None,
    stressor_data: dict[str, Any] | None,
) -> str:
    source_file = entry["file"]
    category = entry["category"]
    file_stem = _slug_from_path(source_file)
    value_type = entry.get("type", "unknown")

    lines = [
        "---",
        f'title: "{_quote_yaml(file_stem)} Data"',
        f'description: "{_quote_yaml(category)} data file documentation"',
        "generated: true",
        "source_files:",
        f'  - "{_quote_yaml(source_file)}"',
        "nav_order: 10",
        "---",
        "",
        f"# {file_stem}",
        "",
        f"ğŸ“„ source (ì¶œì²˜): `{source_file}` | Category (ë¶„ë¥˜): {category} | Type (ìœ í˜•): {value_type}",
        "",
    ]

    lines.extend(_render_overview_section(entry=entry, referenced_by=referenced_by, output_dir=output_dir))
    lines.extend(
        _render_domain_content(
            entry=entry,
            output_dir=output_dir,
            trait_data=trait_data,
            emotion_presets=emotion_presets,
            decay_config=decay_config,
            stressor_data=stressor_data,
        )
    )

    academic_refs = _academic_references(file_stem)
    if academic_refs:
        lines.extend(
            [
                "## ì°¸ê³  ë¬¸í—Œ (Academic References)",
                "",
            ]
        )
        for ref in academic_refs:
            lines.append(f"- {ref}")
        lines.append("")

    lines.append("## ì°¸ì¡°í•˜ëŠ” ì‹œìŠ¤í…œ (Referenced By)")
    lines.append("")
    if referenced_by:
        for ref in referenced_by:
            relative_link = _relative_link(output_dir, ref["module_doc_path"])
            lines.append(f"- [`{ref['module_title']}`]({relative_link}) - {ref['reason']}")
    else:
        lines.append("- None found. (ì°¸ì¡° ì—†ìŒ)")

    lines.extend(
        [
            "",
            "## ìˆ˜ë™ ë…¸íŠ¸ (Manual Notes)",
            "",
            "<!-- MANUAL:START -->",
            "<!-- MANUAL:END -->",
            "",
        ]
    )

    return "\n".join(lines)


def _render_index_page(grouped: dict[str, list[dict[str, Any]]], source_files: list[str]) -> str:
    total_files = sum(len(entries) for entries in grouped.values())

    lines = [
        "---",
        'title: "ë°ì´í„° (Data)"',
        'description: "WorldSim interpreted data file documentation (WorldSim ë°ì´í„° í•´ì„ ë¬¸ì„œ)"',
        "generated: true",
        "source_files:",
    ]

    for source in source_files:
        lines.append(f'  - "{_quote_yaml(source)}"')

    lines.extend(
        [
            "nav_order: 1",
            "---",
            "",
            "# ë°ì´í„° (Data)",
            "",
            f"Total files (ì´ íŒŒì¼ ìˆ˜): **{total_files}**",
            "",
            "## ë„ë©”ì¸ ìš”ì•½ (Domain Summary)",
            "",
            "| Domain (ë„ë©”ì¸) | Files (íŒŒì¼ ìˆ˜) |",
            "|-----------------|----------------|",
        ]
    )

    for group in sorted(grouped.keys()):
        lines.append(f"| {_domain_display(group)} | {len(grouped[group])} |")

    lines.append("")

    for group in sorted(grouped.keys()):
        entries = sorted(grouped[group], key=lambda item: item["file"])
        lines.extend(
            [
                f"## {_domain_display(group)}",
                "",
                "| File (íŒŒì¼) | Category (ë¶„ë¥˜) | Type (ìœ í˜•) | Key Count (í‚¤ ìˆ˜) | Items (í•­ëª© ìˆ˜) |",
                "|-------------|----------------|------------|------------------|----------------|",
            ]
        )

        for entry in entries:
            rel_path = _to_posix(entry["index_rel_link"])
            key_count = entry.get("key_count", "-")
            items = entry.get("items_count", "-")
            file_stem = _slug_from_path(entry["file"])
            value_type = entry.get("type", "unknown")
            category = entry.get("category", "unknown")
            lines.append(
                f"| [{file_stem}]({rel_path}) | {category} | {value_type} | {key_count} | {items} |"
            )

        lines.append("")

    lines.extend(
        [
            "## ìˆ˜ë™ ë…¸íŠ¸ (Manual Notes)",
            "",
            "<!-- MANUAL:START -->",
            "<!-- MANUAL:END -->",
            "",
        ]
    )

    return "\n".join(lines)


def run(manifest: dict, extracted: dict) -> dict:
    """Main entry point.

    Args:
        manifest: The manifest.json data from Phase 1.
        extracted: In-memory extracted payloads.

    Returns:
        dict with keys:
            - files_written: list of output file paths
            - items_processed: int count
            - warnings: list of warning strings
            - errors: list of error strings
    """
    warnings: list[str] = []
    errors: list[str] = []
    files_written: list[str] = []

    if not isinstance(manifest, dict):
        warnings.append("manifest is not an object; using empty manifest")
        manifest = {}

    extracted_payloads = extracted if isinstance(extracted, dict) else {}

    data_entries = _resolve_data_entries(extracted_payloads, warnings, errors)
    if errors:
        return {
            "files_written": files_written,
            "items_processed": 0,
            "warnings": warnings,
            "errors": errors,
        }

    manifest_data_entries = manifest.get("data_files", [])
    if not isinstance(manifest_data_entries, list):
        manifest_data_entries = []

    manifest_meta_by_file: dict[str, dict[str, Any]] = {}
    for entry in manifest_data_entries:
        if not isinstance(entry, dict):
            continue
        file_path = entry.get("file")
        if isinstance(file_path, str) and file_path:
            manifest_meta_by_file[file_path] = entry

    config.ensure_dir(config.CONTENT_DATA)

    referenced_by_map = _build_referenced_by_map(data_entries, manifest, warnings)

    trait_data = _load_optional_payload(extracted_payloads, "trait_data", warnings)
    emotion_presets = _load_optional_payload(extracted_payloads, "emotion_presets", warnings)
    decay_config = _load_optional_payload(extracted_payloads, "decay_config", warnings)
    stressor_data = _load_optional_payload(extracted_payloads, "stressor_data", warnings)

    grouped_for_index: dict[str, list[dict[str, Any]]] = defaultdict(list)
    processed_count = 0

    sorted_entries = sorted(
        (item for item in data_entries if isinstance(item, dict)),
        key=lambda item: (str(item.get("category", "")), str(item.get("file", ""))),
    )

    for entry in sorted_entries:
        source_file = entry.get("file")
        category = entry.get("category")

        if not isinstance(source_file, str) or not source_file:
            warnings.append("skipping data entry with missing file path")
            continue

        if not isinstance(category, str) or not category:
            warnings.append(f"skipping data entry with missing category: {source_file}")
            continue

        normalized_entry = dict(entry)
        raw_content = _entry_content(entry)
        normalized_entry["full_content"] = _filter_meta_content(raw_content)
        normalized_entry["type"] = _json_type(normalized_entry["full_content"])
        if not isinstance(normalized_entry.get("domain"), str) or not normalized_entry.get("domain"):
            normalized_entry["domain"] = _derive_domain_from_file(source_file)

        category_dir = os.path.join(config.CONTENT_DATA, *category.split("/"))
        config.ensure_dir(category_dir)

        output_file = os.path.join(category_dir, f"{_slug_from_path(source_file)}.md")
        page_content = _render_page(
            entry=normalized_entry,
            referenced_by=referenced_by_map.get(source_file, []),
            output_dir=category_dir,
            trait_data=trait_data,
            emotion_presets=emotion_presets,
            decay_config=decay_config,
            stressor_data=stressor_data,
        )
        page_content = _preserve_manual_blocks(output_file, page_content)
        _write_markdown(output_file, page_content, files_written, errors)

        manifest_meta = manifest_meta_by_file.get(source_file, {})
        stats = normalized_entry.get("stats") if isinstance(normalized_entry.get("stats"), dict) else {}

        key_count: int | str = "-"
        if isinstance(manifest_meta, dict):
            keys_count = manifest_meta.get("keys_count")
            if isinstance(keys_count, int):
                key_count = keys_count
            else:
                item_keys = manifest_meta.get("item_keys")
                if isinstance(item_keys, list):
                    key_count = len(item_keys)

        if key_count == "-":
            key_count = stats.get("total_keys", "-")

        items_count = manifest_meta.get("items_count") if isinstance(manifest_meta, dict) else "-"
        if not isinstance(items_count, int):
            items_count = "-"

        index_rel_link = os.path.relpath(output_file, config.CONTENT_DATA)
        grouped_for_index[_domain_group(normalized_entry)].append(
            {
                "file": source_file,
                "category": category,
                "type": normalized_entry.get("type", "unknown"),
                "key_count": key_count,
                "items_count": items_count,
                "index_rel_link": index_rel_link,
            }
        )

        processed_count += 1

    index_path = os.path.join(config.CONTENT_DATA, "_index.md")
    index_sources = ["extracted/data_files.json"]

    optional_sources = {
        "references.json": os.path.join(config.EXTRACTED_DIR, "references.json"),
        "trait_data.json": os.path.join(config.EXTRACTED_DIR, "trait_data.json"),
        "emotion_presets.json": os.path.join(config.EXTRACTED_DIR, "emotion_presets.json"),
        "decay_config.json": os.path.join(config.EXTRACTED_DIR, "decay_config.json"),
        "stressor_data.json": os.path.join(config.EXTRACTED_DIR, "stressor_data.json"),
    }
    for source_name, source_path in optional_sources.items():
        if os.path.exists(source_path):
            index_sources.append(f"extracted/{source_name}")

    index_content = _render_index_page(grouped_for_index, index_sources)
    index_content = _preserve_manual_blocks(index_path, index_content)
    _write_markdown(index_path, index_content, files_written, errors)

    return {
        "files_written": files_written,
        "items_processed": processed_count,
        "warnings": warnings,
        "errors": errors,
    }
